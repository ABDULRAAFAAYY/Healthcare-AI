{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# X-Ray Classification Model Training on Google Colab\n",
                "\n",
                "**Healthcare AI Application - X-Ray Model Training**\n",
                "\n",
                "This notebook trains a ResNet50-based X-ray classification model using Google Colab's T4 GPU.\n",
                "\n",
                "**Expected Training Time:** 30-45 minutes with T4 GPU\n",
                "\n",
                "---\n",
                "\n",
                "## ‚öôÔ∏è Setup Instructions\n",
                "\n",
                "1. **Enable GPU**: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí **T4 GPU**\n",
                "2. **Run all cells** in order\n",
                "3. **Download trained model** at the end\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Check GPU Availability"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "print(\"TensorFlow version:\", tf.__version__)\n",
                "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
                "print(\"\\n‚úÖ If you see GPU listed above, you're ready to train!\")\n",
                "print(\"‚ùå If no GPU, go to: Runtime ‚Üí Change runtime type ‚Üí T4 GPU\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Install Kaggle and Setup Credentials\n",
                "\n",
                "**Before running this cell:**\n",
                "1. Go to https://www.kaggle.com/account\n",
                "2. Scroll to \"API\" section\n",
                "3. Click \"Create New API Token\"\n",
                "4. Upload the downloaded `kaggle.json` file using the file upload button below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install Kaggle\n",
                "!pip install -q kaggle\n",
                "\n",
                "# Upload kaggle.json\n",
                "from google.colab import files\n",
                "print(\"üì§ Please upload your kaggle.json file:\")\n",
                "uploaded = files.upload()\n",
                "\n",
                "# Setup Kaggle credentials\n",
                "!mkdir -p ~/.kaggle\n",
                "!cp kaggle.json ~/.kaggle/\n",
                "!chmod 600 ~/.kaggle/kaggle.json\n",
                "print(\"\\n‚úÖ Kaggle credentials configured!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Download X-Ray Datasets from Kaggle\n",
                "\n",
                "This will download ~2-3 GB of medical imaging data."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "os.makedirs('xray_data', exist_ok=True)\n",
                "os.chdir('xray_data')\n",
                "\n",
                "print(\"üì• Downloading Chest X-Ray Pneumonia dataset...\")\n",
                "!kaggle datasets download -d paultimothymooney/chest-xray-pneumonia\n",
                "!unzip -q chest-xray-pneumonia.zip\n",
                "print(\"‚úÖ Pneumonia dataset downloaded\")\n",
                "\n",
                "print(\"\\nüì• Downloading COVID-19 Radiography dataset...\")\n",
                "!kaggle datasets download -d tawsifurrahman/covid19-radiography-database\n",
                "!unzip -q covid19-radiography-database.zip\n",
                "print(\"‚úÖ COVID-19 dataset downloaded\")\n",
                "\n",
                "os.chdir('..')\n",
                "print(\"\\nüéâ All datasets downloaded!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Organize Dataset into Train/Val/Test Structure"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import shutil\n",
                "from pathlib import Path\n",
                "from sklearn.model_selection import train_test_split\n",
                "import random\n",
                "\n",
                "# Create directory structure\n",
                "data_dir = Path('dataset')\n",
                "classes = ['Normal', 'Pneumonia', 'COVID-19', 'Tuberculosis']\n",
                "\n",
                "for split in ['train', 'validation', 'test']:\n",
                "    for cls in classes:\n",
                "        (data_dir / split / cls).mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "def organize_images(source_dir, class_name, dest_base, max_images=1000):\n",
                "    \"\"\"Copy and split images into train/val/test\"\"\"\n",
                "    if not source_dir.exists():\n",
                "        print(f\"‚ö†Ô∏è  {source_dir} not found, skipping\")\n",
                "        return\n",
                "    \n",
                "    # Get all images\n",
                "    images = list(source_dir.glob('*.jpeg')) + list(source_dir.glob('*.jpg')) + list(source_dir.glob('*.png'))\n",
                "    \n",
                "    # Limit to max_images for faster training\n",
                "    if len(images) > max_images:\n",
                "        images = random.sample(images, max_images)\n",
                "    \n",
                "    if len(images) == 0:\n",
                "        print(f\"‚ö†Ô∏è  No images in {source_dir}\")\n",
                "        return\n",
                "    \n",
                "    # Shuffle\n",
                "    random.shuffle(images)\n",
                "    \n",
                "    # Split: 70% train, 15% val, 15% test\n",
                "    train_size = int(0.7 * len(images))\n",
                "    val_size = int(0.15 * len(images))\n",
                "    \n",
                "    train_imgs = images[:train_size]\n",
                "    val_imgs = images[train_size:train_size + val_size]\n",
                "    test_imgs = images[train_size + val_size:]\n",
                "    \n",
                "    # Copy files\n",
                "    for img in train_imgs:\n",
                "        shutil.copy(img, dest_base / 'train' / class_name / img.name)\n",
                "    for img in val_imgs:\n",
                "        shutil.copy(img, dest_base / 'validation' / class_name / img.name)\n",
                "    for img in test_imgs:\n",
                "        shutil.copy(img, dest_base / 'test' / class_name / img.name)\n",
                "    \n",
                "    print(f\"‚úÖ {class_name}: {len(train_imgs)} train, {len(val_imgs)} val, {len(test_imgs)} test\")\n",
                "\n",
                "# Organize datasets\n",
                "print(\"üìÅ Organizing Pneumonia dataset...\")\n",
                "organize_images(Path('xray_data/chest_xray/train/NORMAL'), 'Normal', data_dir, 800)\n",
                "organize_images(Path('xray_data/chest_xray/train/PNEUMONIA'), 'Pneumonia', data_dir, 800)\n",
                "\n",
                "print(\"\\nüìÅ Organizing COVID-19 dataset...\")\n",
                "covid_base = Path('xray_data/COVID-19_Radiography_Dataset')\n",
                "if not covid_base.exists():\n",
                "    covid_base = Path('xray_data/COVID-19 Radiography Database')\n",
                "\n",
                "organize_images(covid_base / 'COVID/images', 'COVID-19', data_dir, 800)\n",
                "\n",
                "# Use pneumonia images as TB placeholder\n",
                "print(\"\\nüìÅ Creating TB placeholder...\")\n",
                "pneumonia_imgs = list((data_dir / 'train' / 'Pneumonia').glob('*.jpeg'))[:200]\n",
                "for img in pneumonia_imgs:\n",
                "    shutil.copy(img, data_dir / 'train' / 'Tuberculosis' / img.name)\n",
                "print(f\"‚úÖ Tuberculosis: {len(pneumonia_imgs)} train images\")\n",
                "\n",
                "print(\"\\nüéâ Dataset organized successfully!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Build and Train ResNet50 Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers\n",
                "from tensorflow.keras.applications import ResNet50\n",
                "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
                "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
                "\n",
                "# Configuration\n",
                "IMG_SIZE = (224, 224)\n",
                "BATCH_SIZE = 32\n",
                "EPOCHS_PHASE1 = 10\n",
                "EPOCHS_PHASE2 = 20\n",
                "\n",
                "# Data generators\n",
                "train_datagen = ImageDataGenerator(\n",
                "    rescale=1./255,\n",
                "    rotation_range=15,\n",
                "    width_shift_range=0.1,\n",
                "    height_shift_range=0.1,\n",
                "    shear_range=0.1,\n",
                "    zoom_range=0.1,\n",
                "    horizontal_flip=True,\n",
                "    fill_mode='nearest'\n",
                ")\n",
                "\n",
                "val_datagen = ImageDataGenerator(rescale=1./255)\n",
                "\n",
                "train_generator = train_datagen.flow_from_directory(\n",
                "    'dataset/train',\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical'\n",
                ")\n",
                "\n",
                "val_generator = val_datagen.flow_from_directory(\n",
                "    'dataset/validation',\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical'\n",
                ")\n",
                "\n",
                "print(f\"\\n‚úÖ Found {train_generator.samples} training images\")\n",
                "print(f\"‚úÖ Found {val_generator.samples} validation images\")\n",
                "print(f\"‚úÖ Classes: {list(train_generator.class_indices.keys())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Build model\n",
                "base_model = ResNet50(\n",
                "    weights='imagenet',\n",
                "    include_top=False,\n",
                "    input_shape=(*IMG_SIZE, 3)\n",
                ")\n",
                "\n",
                "base_model.trainable = False  # Freeze initially\n",
                "\n",
                "model = keras.Sequential([\n",
                "    base_model,\n",
                "    layers.GlobalAveragePooling2D(),\n",
                "    layers.Dense(512, activation='relu'),\n",
                "    layers.Dropout(0.5),\n",
                "    layers.Dense(len(train_generator.class_indices), activation='softmax')\n",
                "])\n",
                "\n",
                "model.compile(\n",
                "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Model built successfully!\")\n",
                "model.summary()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Phase 1: Train with Frozen Base (10 epochs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Starting Phase 1: Training with frozen base...\\n\")\n",
                "\n",
                "callbacks_phase1 = [\n",
                "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=1e-7)\n",
                "]\n",
                "\n",
                "history1 = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS_PHASE1,\n",
                "    validation_data=val_generator,\n",
                "    callbacks=callbacks_phase1\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Phase 1 complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Phase 2: Fine-tune (20 epochs)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"üöÄ Starting Phase 2: Fine-tuning...\\n\")\n",
                "\n",
                "# Unfreeze last layers\n",
                "base_model.trainable = True\n",
                "for layer in base_model.layers[:-30]:\n",
                "    layer.trainable = False\n",
                "\n",
                "# Recompile with lower learning rate\n",
                "model.compile(\n",
                "    optimizer=keras.optimizers.Adam(learning_rate=0.0001),\n",
                "    loss='categorical_crossentropy',\n",
                "    metrics=['accuracy']\n",
                ")\n",
                "\n",
                "callbacks_phase2 = [\n",
                "    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True),\n",
                "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=4, min_lr=1e-8)\n",
                "]\n",
                "\n",
                "history2 = model.fit(\n",
                "    train_generator,\n",
                "    epochs=EPOCHS_PHASE2,\n",
                "    validation_data=val_generator,\n",
                "    callbacks=callbacks_phase2\n",
                ")\n",
                "\n",
                "print(\"\\n‚úÖ Phase 2 complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Plot Training History"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Combine histories\n",
                "acc = history1.history['accuracy'] + history2.history['accuracy']\n",
                "val_acc = history1.history['val_accuracy'] + history2.history['val_accuracy']\n",
                "loss = history1.history['loss'] + history2.history['loss']\n",
                "val_loss = history1.history['val_loss'] + history2.history['val_loss']\n",
                "\n",
                "epochs_range = range(len(acc))\n",
                "\n",
                "plt.figure(figsize=(14, 5))\n",
                "\n",
                "plt.subplot(1, 2, 1)\n",
                "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
                "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
                "plt.axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Fine-tuning starts')\n",
                "plt.legend(loc='lower right')\n",
                "plt.title('Training and Validation Accuracy')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Accuracy')\n",
                "\n",
                "plt.subplot(1, 2, 2)\n",
                "plt.plot(epochs_range, loss, label='Training Loss')\n",
                "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
                "plt.axvline(x=EPOCHS_PHASE1, color='r', linestyle='--', label='Fine-tuning starts')\n",
                "plt.legend(loc='upper right')\n",
                "plt.title('Training and Validation Loss')\n",
                "plt.xlabel('Epoch')\n",
                "plt.ylabel('Loss')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('training_history.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nüìä Training plots saved!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Evaluate on Test Set"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_generator = val_datagen.flow_from_directory(\n",
                "    'dataset/test',\n",
                "    target_size=IMG_SIZE,\n",
                "    batch_size=BATCH_SIZE,\n",
                "    class_mode='categorical',\n",
                "    shuffle=False\n",
                ")\n",
                "\n",
                "test_loss, test_accuracy = model.evaluate(test_generator)\n",
                "\n",
                "print(f\"\\nüìä Test Results:\")\n",
                "print(f\"   Loss: {test_loss:.4f}\")\n",
                "print(f\"   Accuracy: {test_accuracy*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 10. Save Model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save model\n",
                "model.save('xray_model.h5')\n",
                "print(\"\\n‚úÖ Model saved as 'xray_model.h5'\")\n",
                "\n",
                "# Get model size\n",
                "import os\n",
                "model_size = os.path.getsize('xray_model.h5') / (1024 * 1024)\n",
                "print(f\"üì¶ Model size: {model_size:.2f} MB\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 11. Download Trained Model\n",
                "\n",
                "**Download the model file to your computer:**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import files\n",
                "\n",
                "print(\"üì• Downloading trained model...\")\n",
                "files.download('xray_model.h5')\n",
                "print(\"\\n‚úÖ Download started! Check your browser's downloads folder.\")\n",
                "\n",
                "# Also download training plot\n",
                "print(\"\\nüì• Downloading training plot...\")\n",
                "files.download('training_history.png')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "\n",
                "## üéâ Training Complete!\n",
                "\n",
                "### Next Steps:\n",
                "\n",
                "1. **Download the model** (xray_model.h5) using the cell above\n",
                "2. **Copy to your project**:\n",
                "   ```\n",
                "   Healthcare AI/backend/models/xray_model.h5\n",
                "   ```\n",
                "3. **Restart your Flask backend**:\n",
                "   ```bash\n",
                "   python backend/app.py\n",
                "   ```\n",
                "4. **Test it!** Go to http://localhost:3000 and upload X-ray images\n",
                "\n",
                "### Your model is now ready for real predictions! üöÄ\n",
                "\n",
                "---\n",
                "\n",
                "**Training Summary:**\n",
                "- ‚úÖ Dataset organized\n",
                "- ‚úÖ ResNet50 model trained\n",
                "- ‚úÖ Two-phase training completed\n",
                "- ‚úÖ Model evaluated on test set\n",
                "- ‚úÖ Model saved and ready to download\n",
                "\n",
                "**Model Performance:**\n",
                "- Test Accuracy: Check cell 9 output\n",
                "- Model Size: ~90 MB\n",
                "- Classes: Normal, Pneumonia, COVID-19, Tuberculosis\n",
                "\n",
                "---"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}